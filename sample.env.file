# --- LLM API Keys ---
# IMPORTANT: Replace placeholder keys with your actual API keys!
# Only include the keys for the providers you intend to use.
# Copy this file to .env and fill in your actual keys.

# OpenAI (provider="openai")
# OPENAI_API_KEY=sk-YourActualOpenAIKeyHereXXXXXXXXXXXXXXXXXXXXXX
# OPENAI_MODEL_NAME=gpt-4o

# Anthropic (provider="anthropic")
# ANTHROPIC_API_KEY=sk-ant-YourActualAnthropicKeyHereXXXXXXXXXXXXXXXXX
# ANTHROPIC_MODEL_NAME=claude-3-opus

# Google Generative AI (Gemini) (provider="gemini")
# GOOGLE_API_KEY=AIzaSyYourActualGoogleApiKeyHereXXXXXXXXXXXXXXXX
# GOOGLE_GENAI_MODEL_NAME=gemini-2.5-pro

# OpenRouter (provider="openrouter")
# OPENROUTER_API_KEY=sk-or-v1-YourActualOpenRouterKeyHereXXXXXXXXXXXXXXXX
# OPENROUTER_MODEL_NAME=google/gemini-2.5-pro

# Mistral AI (provider="mistral")
# MISTRAL_API_KEY=YourActualMistralApiKeyHereXXXXXXXXXXXXXXXXXXXX
# MISTRAL_MODEL_NAME=mistral-large

# DeepSeek (provider="deepseek")
# DEEPSEEK_API_KEY=YourActualDeepSeekKeyHereXXXXXXXXXXXXXXXXX
# DEEPSEEK_MODEL_NAME=deepseek-chat

# Ollama (provider="ollama") - No API key needed by default
# OLLAMA_MODEL_NAME=llama3

# LocalAI (provider="localai")
# LOCALAI_API_KEY=your-localai-api-key-here
# LOCALAI_BASE_URL=http://localhost:8083/v1
# LOCALAI_MODEL_NAME=gpt-3.5-turbo

# --- Optional Provider Base URLs ---
# Override the default API endpoint for a provider. Useful for proxies, local models, etc.

# OpenAI (Defaults to https://api.openai.com/v1 if not set)
# OPENAI_BASE_URL=http://localhost:1234/v1

# Anthropic (Defaults to https://api.anthropic.com if not set)
# ANTHROPIC_BASE_URL=YourAnthropicProxyURL

# Google GenAI - Base URL is not configurable via this variable.

# OpenRouter (Defaults to https://openrouter.ai/api/v1 if not set)
# OPENROUTER_BASE_URL=YourOpenRouterProxyURL

# Mistral AI (Defaults to official Mistral endpoint if not set)
# MISTRAL_BASE_URL=YourMistralProxyURL # Note: Corresponds to 'endpoint' parameter in ChatMistralAI

# DeepSeek (Defaults to https://api.deepseek.com/v1 if not set)
# DEEPSEEK_BASE_URL=YourDeepSeekProxyURL

# Ollama (Defaults to http://localhost:11434 if not set)
# OLLAMA_BASE_URL=http://other-ollama-host:11434

# --- Application Settings (Optional) ---

# Chunking Settings
# MAX_CHUNK_SIZE=2000 # Approximate maximum characters per chunk for translation.
# MIN_CHUNK_SIZE=100  # Chunks smaller than this will be merged with adjacent chunks if possible.

# Parallel Processing Settings
# MAX_PARALLEL_WORKERS=4 # Number of chunks to translate concurrently.
